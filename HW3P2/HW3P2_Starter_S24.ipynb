{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78ZTCIXoof2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from torchsummaryX import summary\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "# import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation \n",
        "# `ctcdecode` is a obselete package, it won't go through compilation on any mordern compilers\n",
        "# import ctcdecode\n",
        "import Levenshtein\n",
        "# from ctcdecode import CTCBeamDecoder\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eN2kcxwXLLBb"
      },
      "outputs": [],
      "source": [
        "\n",
        "DATA_ROOT = \"/mnt/e/Workspace/IDL/Data/hw3/11-785-s24-hw3p2\"\n",
        "MODEL_ROOT = \"/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw3p2/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root=DATA_ROOT, partition=\"train-clean-100\", use_cmn=False, audio_transformation=None):\n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "        \n",
        "        self.phonemes = PHONEMES\n",
        "\n",
        "        self.mfccs, self.transcripts = self._init_data(f\"{root}/{partition}\", use_cmn=use_cmn)\n",
        "        \n",
        "        print(self.length, len(self.mfccs), len(self.transcripts))\n",
        "\n",
        "        if audio_transformation is not None:\n",
        "            self.transformation = audio_transformation\n",
        "        else:\n",
        "            self.transformation = nn.Sequential()\n",
        "\n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "        \n",
        "    def _init_data(self, root: str, use_cmn = False):\n",
        "        self.mfcc_dir       = f\"{root}/mfcc\"\n",
        "        self.transcript_dir = f\"{root}/transcript\"\n",
        "        mfcc_names          = os.listdir(self.mfcc_dir)\n",
        "        transcript_names    = os.listdir(self.transcript_dir)\n",
        "        \n",
        "        self.length = len(mfcc_names)\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        for i in tqdm(range(len(mfcc_names))):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            if use_cmn:\n",
        "                mfcc = mfcc - np.mean(mfcc, axis=0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = np.load(f\"{self.transcript_dir}/{transcript_names[i]}\") \n",
        "            # Remove [SOS] and [EOS] from the transcript\n",
        "            assert transcript[0] == '[SOS]' and transcript[-1] == '[EOS]'\n",
        "            transcript = transcript[1:-1]\n",
        "            #lookup phoneme index\n",
        "            transcript = np.vectorize(self.phonemes.index)(transcript)\n",
        "            # assert len(mfcc) == len(transcript)\n",
        "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
        "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "            \n",
        "        # return np.concatenate(self.mfccs, axis=0), np.concatenate(self.transcripts, axis=0)\n",
        "        return self.mfccs, self.transcripts\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = self.mfccs[ind]\n",
        "        transcript = self.transcripts[ind]\n",
        "        return torch.FloatTensor(mfcc), torch.tensor(transcript)\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features,\n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc, batch_transcript, = [], []\n",
        "        lengths_mfcc, lengths_transcript = [], []\n",
        "        for (m, t) in batch:\n",
        "          batch_mfcc.append(m)\n",
        "          lengths_mfcc.append(len(m))\n",
        "          batch_transcript.append(t)\n",
        "          lengths_transcript.append(len(t))\n",
        "        \n",
        "       \n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True)\n",
        "        \n",
        "        batch_mfcc_pad = self.transformation(batch_mfcc_pad)\n",
        "        \n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True)\n",
        "        \n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "# TODO\n",
        "class AudioDatasetTest(AudioDataset):\n",
        "      def __init__(self, root=DATA_ROOT, partition=\"test-clean\", use_cmn=True, audio_transformation=None):\n",
        "            super().__init__(root, partition, use_cmn, audio_transformation=None)\n",
        "      \n",
        "      def _init_data(self, root: str, use_cmn):\n",
        "            self.mfcc_dir = f\"{root}/mfcc\"\n",
        "\n",
        "            mfcc_names = os.listdir(self.mfcc_dir)\n",
        "            \n",
        "            self.length = len(mfcc_names)\n",
        "\n",
        "            self.mfccs, self.transcripts = [], []\n",
        "\n",
        "            for i in tqdm(range(len(mfcc_names))):\n",
        "                  #   Load a single mfcc\n",
        "                  mfcc = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
        "                  transcript = np.array([0 for _ in range(len(mfcc))])\n",
        "\n",
        "                  assert len(mfcc) == len(transcript)\n",
        "\n",
        "                  self.mfccs.append(mfcc)\n",
        "                  self.transcripts.append(transcript)\n",
        "\n",
        "            return np.concatenate(self.mfccs, axis=0), np.concatenate(self.transcripts, axis=0)\n",
        "      \n",
        "      def __getitem__(self, ind):\n",
        "            mfcc = self.mfccs[ind]\n",
        "            return torch.FloatTensor(mfcc)\n",
        "      \n",
        "      def collate_fn(self, batch):\n",
        "            batch_mfcc = []\n",
        "            lengths_mfcc = []\n",
        "        \n",
        "            for mfcc in batch:\n",
        "                  batch_mfcc.append(mfcc)\n",
        "            \n",
        "                  lengths_mfcc.append(len(mfcc))\n",
        "            \n",
        "      \n",
        "            batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True)\n",
        "            \n",
        "\n",
        "            # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "            # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "            #                  -> Would we apply transformation on the validation set as well?\n",
        "            #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "            \n",
        "            # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "            return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Config - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "root = '/content/11-785-s24-hw3p2/'\n",
        "\n",
        "# Feel free to add more items here\n",
        "config = {\n",
        "    \"beam_width\" : 5,\n",
        "    \"lr\"         : 2e-3,\n",
        "    \"epochs\"     : 50,\n",
        "    \"batch_size\" : 256,  # Increase if your device can handle it\n",
        "    \"dropout\": 0.2\n",
        "}\n",
        "\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "transforms = nn.Sequential(\n",
        "            PermuteBlock(),\n",
        "            tat.FrequencyMasking(freq_mask_param=5),\n",
        "            tat.TimeMasking(time_mask_param=100),\n",
        "            PermuteBlock()\n",
        " ) # set of tranformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3_kG0gU2x4hH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2443"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!!\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28539/28539 [03:07<00:00, 152.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28539 28539 28539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2703/2703 [00:16<00:00, 159.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2703 2703 2703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2620/2620 [00:08<00:00, 306.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2620 (1934138, 27) (1934138,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(partition=\"train-clean-100\", use_cmn=True, audio_transformation=transforms) \n",
        "val_data = AudioDataset(partition=\"dev-clean\", use_cmn=True, audio_transformation=None)\n",
        "test_data = AudioDatasetTest(partition=\"test-clean\", use_cmn=True, audio_transformation=None)\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 8,\n",
        "    batch_size  = config[\"batch_size\"], \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn = train_data.collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 4,\n",
        "    batch_size  = config[\"batch_size\"], \n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn = val_data.collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data, \n",
        "    num_workers = 4,\n",
        "    batch_size  = 1, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn = test_data.collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  256\n",
            "Train dataset samples = 28539, batches = 112\n",
            "Val dataset samples = 2703, batches = 11\n",
            "Test dataset samples = 2620, batches = 2620\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "\n",
        "print(\"Batch size: \", config[\"batch_size\"])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cXMtwyviKaxK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256, 1686, 27]) torch.Size([256, 201]) torch.Size([256]) torch.Size([256])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-qb7wnAzCZl"
      },
      "source": [
        "## ASR Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "### Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.blstm = nn.LSTM(input_size = 2*input_size, hidden_size = hidden_size, num_layers = 1, bidirectional = True, dropout = 0.2, batch_first = True) \n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "        \n",
        "        x , lengths = pad_packed_sequence(x_packed, batch_first = True)\n",
        "        \n",
        "        x, x_lens = self.trunc_reshape(x, lengths)\n",
        "        \n",
        "        x = pack_padded_sequence(x, x_lens, batch_first = True, enforce_sorted= False)\n",
        "        \n",
        "        x , h= self.blstm(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        \n",
        "        if x.shape[1]%2 != 0:\n",
        "            x= x[:,:-1,:]\n",
        "\n",
        "        x = x.reshape(x.shape[0],x.shape[1]//2, x.shape[2]*2)\n",
        "        x_lens  = x_lens//2\n",
        "        return x, x_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "from torchnlp.nn import LockedDropout\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        expand_dims = [128, 256]\n",
        "                \n",
        "        self.embed = nn.Sequential(\n",
        "            PermuteBlock(), \n",
        "            nn.Conv1d(in_channels=input_size, out_channels=expand_dims[0], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(num_features=expand_dims[0]),\n",
        "            nn.Mish(),\n",
        "            nn.Conv1d(in_channels=expand_dims[0], out_channels=expand_dims[1], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(num_features=expand_dims[1]),\n",
        "            PermuteBlock())\n",
        "\n",
        "        self.pBLSTMs = nn.Sequential(\n",
        "            pBLSTM(input_size=expand_dims[1], hidden_size=hidden_size),\n",
        "            LockedDropout(0.4),\n",
        "            pBLSTM(input_size=2*hidden_size, hidden_size=hidden_size),\n",
        "            LockedDropout(0.3)\n",
        "        )\n",
        "         \n",
        "    def forward(self, x, lens):\n",
        "        x = self.embed(x)      \n",
        "        lens = lens.clamp(max=x.shape[1]).cpu()\n",
        "        \n",
        "        x = pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
        "        x = self.pBLSTMs(x)\n",
        "        outputs, lens = pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        return outputs, lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class DynamicMlpNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_sizes, dropout_rate):\n",
        "        super(DynamicMlpNet, self).__init__()\n",
        "        self.layers = []\n",
        "        for i, hs in enumerate(hidden_sizes):\n",
        "            self.layers.extend(self._mlp_layer_provider(input_size, hs, dropout_rate, i))\n",
        "            input_size = hs\n",
        "        self.layers.append(nn.Linear(input_size, output_size)) # output\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return out\n",
        "    \n",
        "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate, idx) -> list[nn.Module]:\n",
        "        return [\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ]\n",
        "    \n",
        "    def describe() -> str:\n",
        "        return \"base model, don't use this\"\n",
        "\n",
        "class Decoder(DynamicMlpNet):\n",
        "\n",
        "    def __init__(self, embed_size, output_size= 41):\n",
        "        super().__init__(input_size=embed_size, output_size=output_size, dropout_rate=config[\"dropout\"], hidden_sizes=[2048,1024])\n",
        "                \n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "    \n",
        "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate, idx) -> list[nn.Module]:\n",
        "        return [\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Mish(),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(hidden_size), PermuteBlock(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        ] if idx != 0 else [\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(input_size), PermuteBlock(),\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Mish(),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(hidden_size), PermuteBlock(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        ]\n",
        "    \n",
        "    def forward(self, encoder_out):\n",
        "        out = self.model(encoder_out)\n",
        "        out = self.softmax(out)\n",
        "        return out \n",
        "    \n",
        "    def describe() -> str:\n",
        "        return \"MLP Decoder model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.augmentations  = torch.nn.Sequential(\n",
        "        #     #TODO Add Time Masking/ Frequency Masking\n",
        "        #     #Hint: See how to use PermuteBlock() function defined above\n",
        "        #     PermuteBlock(), \n",
        "        #     torchaudio.transforms.FrequencyMasking(freq_mask_param=10),\n",
        "        #     torchaudio.transforms.TimeMasking(time_mask_param=100),\n",
        "        #     PermuteBlock(),\n",
        "        # ) # did augmentation in the collate_fn\n",
        "        self.encoder        = Encoder(input_size, embed_size)# TODO: Initialize Encoder\n",
        "        self.decoder        = Decoder(embed_size*2, output_size) # TODO: Initialize Decoder \n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self, x, lengths_x):\n",
        "        \n",
        "        # if self.training:\n",
        "        #     x = self.augmentations(x)\n",
        "\n",
        "        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
        "        decoder_out                 = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## Initialize ASR Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "oaaDsnnLMUz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASRModel(\n",
            "  (encoder): Encoder(\n",
            "    (embed): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): Conv1d(27, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): Mish()\n",
            "      (4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): PermuteBlock()\n",
            "    )\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(512, 512, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "      )\n",
            "      (1): LockedDropout(p=0.4)\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(2048, 512, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "      )\n",
            "      (3): LockedDropout(p=0.3)\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (model): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): PermuteBlock()\n",
            "      (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "      (4): Mish()\n",
            "      (5): PermuteBlock()\n",
            "      (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (7): PermuteBlock()\n",
            "      (8): Dropout(p=0.2, inplace=False)\n",
            "      (9): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "      (10): Mish()\n",
            "      (11): PermuteBlock()\n",
            "      (12): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (13): PermuteBlock()\n",
            "      (14): Dropout(p=0.2, inplace=False)\n",
            "      (15): Linear(in_features=1024, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ASRModel                                 [256, 421, 41]            --\n",
              "├─Encoder: 1-1                           [256, 421, 1024]          --\n",
              "│    └─Sequential: 2-1                   [256, 1686, 256]          --\n",
              "│    │    └─PermuteBlock: 3-1            [256, 27, 1686]           --\n",
              "│    │    └─Conv1d: 3-2                  [256, 128, 1686]          10,496\n",
              "│    │    └─BatchNorm1d: 3-3             [256, 128, 1686]          256\n",
              "│    │    └─Mish: 3-4                    [256, 128, 1686]          --\n",
              "│    │    └─Conv1d: 3-5                  [256, 256, 1686]          98,560\n",
              "│    │    └─BatchNorm1d: 3-6             [256, 256, 1686]          512\n",
              "│    │    └─PermuteBlock: 3-7            [256, 1686, 256]          --\n",
              "│    └─Sequential: 2-2                   [80940, 1024]             --\n",
              "│    │    └─pBLSTM: 3-8                  [162003, 1024]            4,202,496\n",
              "│    │    └─LockedDropout: 3-9           [162003, 1024]            --\n",
              "│    │    └─pBLSTM: 3-10                 [80940, 1024]             10,493,952\n",
              "│    │    └─LockedDropout: 3-11          [80940, 1024]             --\n",
              "├─Decoder: 1-2                           [256, 421, 41]            --\n",
              "│    └─Sequential: 2-3                   [256, 421, 41]            --\n",
              "│    │    └─PermuteBlock: 3-12           [256, 1024, 421]          --\n",
              "│    │    └─BatchNorm1d: 3-13            [256, 1024, 421]          2,048\n",
              "│    │    └─PermuteBlock: 3-14           [256, 421, 1024]          --\n",
              "│    │    └─Linear: 3-15                 [256, 421, 2048]          2,099,200\n",
              "│    │    └─Mish: 3-16                   [256, 421, 2048]          --\n",
              "│    │    └─PermuteBlock: 3-17           [256, 2048, 421]          --\n",
              "│    │    └─BatchNorm1d: 3-18            [256, 2048, 421]          4,096\n",
              "│    │    └─PermuteBlock: 3-19           [256, 421, 2048]          --\n",
              "│    │    └─Dropout: 3-20                [256, 421, 2048]          --\n",
              "│    │    └─Linear: 3-21                 [256, 421, 1024]          2,098,176\n",
              "│    │    └─Mish: 3-22                   [256, 421, 1024]          --\n",
              "│    │    └─PermuteBlock: 3-23           [256, 1024, 421]          --\n",
              "│    │    └─BatchNorm1d: 3-24            [256, 1024, 421]          2,048\n",
              "│    │    └─PermuteBlock: 3-25           [256, 421, 1024]          --\n",
              "│    │    └─Dropout: 3-26                [256, 421, 1024]          --\n",
              "│    │    └─Linear: 3-27                 [256, 421, 41]            42,025\n",
              "│    └─LogSoftmax: 2-4                   [256, 421, 41]            --\n",
              "==========================================================================================\n",
              "Total params: 19,053,865\n",
              "Trainable params: 19,053,865\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.TERABYTES): 1,566.97\n",
              "==========================================================================================\n",
              "Input size (MB): 46.62\n",
              "Forward/backward pass size (MB): 10857.70\n",
              "Params size (MB): 76.22\n",
              "Estimated Total Size (MB): 10980.53\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ASRModel(\n",
        "    input_size  = 27,\n",
        "    embed_size  = 512,\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "summary(model, input_data=[x.to(device), lx.to(device)], device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config\n",
        "Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchaudio\n",
        "torchaudio.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Unknown entry in dictionary: ''",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[85], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# What goes in here?\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# decoder = CTCBeamDecoder(LABELS, beam_width = config[\"beam_width\"], log_probs_input = True)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m decoder \u001b[38;5;241m=\u001b[39m \u001b[43mctc_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlexicon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLABELS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeam_width\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msil_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblank_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 3, threshold=1e-2)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, eta_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/11785/lib/python3.11/site-packages/torchaudio/models/decoder/_ctc_decoder.py:512\u001b[0m, in \u001b[0;36mctc_decoder\u001b[0;34m(lexicon, tokens, lm, lm_dict, nbest, beam_size, beam_size_token, beam_threshold, lm_weight, word_score, unk_score, sil_score, log_add, blank_token, sil_token, unk_word)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     lm \u001b[38;5;241m=\u001b[39m _ZeroLM()\n\u001b[0;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCTCDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnbest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlexicon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlexicon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblank_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblank_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43msil_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msil_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/11785/lib/python3.11/site-packages/torchaudio/models/decoder/_ctc_decoder.py:220\u001b[0m, in \u001b[0;36mCTCDecoder.__init__\u001b[0;34m(self, nbest, lexicon, word_dict, tokens_dict, lm, decoder_options, blank_token, sil_token, unk_word)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_dict \u001b[38;5;241m=\u001b[39m word_dict\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_dict \u001b[38;5;241m=\u001b[39m tokens_dict\n\u001b[0;32m--> 220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblank_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m silence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_dict\u001b[38;5;241m.\u001b[39mget_index(sil_token)\n\u001b[1;32m    222\u001b[0m transitions \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown entry in dictionary: ''"
          ]
        }
      ],
      "source": [
        "from torchaudio.models.decoder import ctc_decoder\n",
        "from pyctcdecode import build_ctcdecoder\n",
        "\n",
        "\n",
        "criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False) # Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr= config['lr']) # What goes in here?\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "\n",
        "# decoder = CTCBeamDecoder(LABELS, beam_width = config[\"beam_width\"], log_probs_input = True)\n",
        "decoder = build_ctcdecoder(\n",
        "    LABELS,\n",
        "    \n",
        ")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 3, threshold=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 50, eta_min = 1e-6)\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "# Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    \n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings  = []\n",
        "    # print(beam_results.shape)\n",
        "    # print(beam_results)\n",
        "    for i in range(output_lens.shape[0]):\n",
        "        #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "        pred_strings.append(''.join([PHONEME_MAP[n] for n in beam_results[i][0][:out_seq_len[i][0]]]))\n",
        "    # print(pred_strings)\n",
        "    \n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "    \n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    # print(batch_size)\n",
        "    for i in range(batch_size):\n",
        "        # TODO: Get predicted string and label string for each element in the batch\n",
        "        pred_string = pred_strings[i]#TODO\n",
        "        # print('pred',pred_string)\n",
        "        label_string = ''.join([PHONEME_MAP[n] for n in label[i][:label_lens[i]]]) #TODO\n",
        "        # print('label',label_string)\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    \n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qk9iZud1LXT"
      },
      "source": [
        "# Test Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnTLL-5gMBrY"
      },
      "outputs": [],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "# WandB\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiDduMaDIARE"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"<replace with your API key here>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s52yBOvICPZ"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16.\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = #TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = #TODO set best model path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR43E28rM9Ak"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = #TODO\n",
        "\n",
        "    train_loss              = #TODO\n",
        "    valid_loss, valid_dist  = #TODO\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_dist': valid_dist,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "outputs": [],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = #TODO\n",
        "\n",
        "test_decoder    = #TODO\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string= # TODO call decode_prediction\n",
        "    #TODO save the output in results array.\n",
        "\n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d70dvu_lsMlv"
      },
      "outputs": [],
      "source": [
        "data_dir = f\"{root}/test-clean/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results\n",
        "df.to_csv('submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1sZmEIs4yIz"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c hw3p2asr-s24 -f submission.csv -m \"I made it!\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HLad4pChcuvX",
        "rd5aNaLVoR_g",
        "qpYExu4vT4_g",
        "M2H4EEj-sD32"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
