{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Y519Dc7b3HMi","executionInfo":{"status":"ok","timestamp":1707197579486,"user_tz":300,"elapsed":12167,"user":{"displayName":"Ishan Mamadapur","userId":"15818966938469395733"}},"outputId":"2725927b-fd75-4728-8d92-547c304b7682","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Device:  cpu\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchsummary import summary\n","import torchvision #This library is used for image-based operations (Augmentations)\n","\n","import os\n","import gc\n","from tqdm import tqdm\n","import math\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","import glob\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", DEVICE)"]},{"cell_type":"markdown","source":["# ArcFace Loss\n","\n","[ArcFace: Additive Angular Margin Loss for Deep\n","Face Recognition](https://arxiv.org/pdf/1801.07698.pdf) [equation 3]\n","\n","ArcFace Loss is trying to maximize the geodesic distance on the hypersphere between features of different classes to make the features more separately. Here is a blog that explains ArcFace Loss in detail: [link](https://medium.com/analytics-vidhya/face-recognition-and-arcface-additive-angular-margin-loss-for-deep-face-recognition-44abc56916c#:~:text=The%20ArcFace%20loss%20maximizes%20the,implemented%20with%20negligible%20computational%20overhead)\n","\n","$$L_{afl} = - log \\frac{e^{scos(\\theta_{y_i} + m)}}{e^{s cos(\\theta_{y_i} + m)} + \\sum_{j=1,j \\neq y_i}^N e^{s cos(\\theta_j)}}$$\n","\n","Play around with the `margin` and `scaler` hyperparameters as they are instrumental to the performance of this loss in fine tuning your model.\n","\n"],"metadata":{"id":"xR8bGXJ0sM20"}},{"cell_type":"code","source":["class ArcFaceModel(torch.nn.Module):\n","    '''\n","    To train in a standard training loop make sure to modify the train function so you pass in the inputs and the labels\n","    i.e. output = model(images, labels)\n","    Experiment with different values of margin and scaler\n","    '''\n","    def __init__(self, model, margin=0.5, scaler=64, embedding_size=NotImplemented, num_classes=NotImplemented):\n","        super(ArcFaceModel, self).__init__()\n","        self.embedding_size = embedding_size\n","        self.num_classes = num_classes\n","\n","        # small number to avoid invalid arcCos values\n","        self.eps = 1e-7\n","\n","        # hyperparameters\n","        self.margin = margin\n","        self.scaler = scaler\n","\n","        # load classification model\n","        self.model = model\n","\n","        # Initializing the arcface linear layer with the weights of the classifier from the trained CNN\n","        self.AFL_linear = torch.nn.Linear(embedding_size, num_classes, bias=False) # Why set bias=False? Check out the paper.\n","        with torch.no_grad():\n","          self.AFL_linear.weight.copy_(self.model.cls_layer.weight)\n","\n","        # Initializing utility functions for normalization, arcCos, cos and onehot encoding\n","        self.normalizer = torch.nn.functional.normalize\n","        self.arcCos = torch.acos\n","        self.cos = torch.cos\n","        self.one_hot = torch.nn.functional.one_hot\n","\n","\n","    def forward(self, x, labels):\n","        # Get face embedding. Note that we pass return_feats=True to get the image's features and not the final logits.\n","        embedding = self.model(x, return_feats=True)\n","\n","        # TODO: normalize face embedding\n","        embedding = NotImplemented\n","\n","        # TODO: normalize linear layer weights.\n","        # NOTE: The normalized weights need to be wrapped in torch.nn.Parameter before assigning to AFL_linear.\n","        with torch.no_grad():\n","          self.AFL_linear.weight = torch.nn.Parameter(NotImplemented)\n","\n","        # TODO: take dot product to get cos theta, remember that Wx = ||W||||x||cos(\\theta) and ||W|| = 1, ||x|| = 1\n","        cosine = NotImplemented\n","\n","        # We clamp the values to be a little higher than -1 and a little lower than one so we don't get nan values when we call arccos\n","        cosine = torch.clamp(cosine, min=-1.0+self.eps, max=1.0-self.eps)\n","\n","        # TODO: get theta by performing arccos(cos(theta))\n","        theta = NotImplemented\n","\n","        # TODO: convert labels to one-hot\n","        one_hot_labels = NotImplemented\n","        # TODO: create a mask with m at positions with label 1 and 0 at positions with label 0\n","        margin_mask = NotImplemented\n","        # TODO: add margin m to theta\n","        theta_m = NotImplemented\n","\n","        # calculate the cosine value for theta with margin added and scale with self.scaler\n","        logits = NotImplemented # this value is then passed to crossEntropyLoss in train loop to calculate arcface loss\n","\n","        return logits"],"metadata":{"id":"tv2R3qe9Gmwq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SphereFace Loss\n","[SphereFace: Deep Hypersphere Embedding for Face Recognition](https://arxiv.org/pdf/1704.08063.pdf)\n","\n","[SphereFace Revived:\n","Unifying Hyperspherical Face Recognition](https://arxiv.org/pdf/2109.05565.pdf)\n","\n","$$L_{sfl} = - log \\frac{e^{scos(m\\theta_{y_i})}}{e^{s cos(m\\theta_{y_i})} + \\sum_{j=1,j \\neq y_i}^N e^{s cos(\\theta_j)}}$$\n","\n","Notice that the only difference between arcface loss and sphere loss is from $e^{scos(\\theta_{y_i} + m)}$ to $e^{scos(m\\theta_{y_i})}$. You should be able to implement this based on the comments in ArcFace loss and update `margin_mask` variable accordingly.\n","\n","Play around with the `margin` and `scaler` hyperparameters as they are instrumental to the performance of this loss in fine tuning your model.\n","\n","Please note that this is a basic version of SphereFace loss. As you can read in the above listed papers, there are several modifications you can make to it.\n"],"metadata":{"id":"JgYWY_b3reVr"}},{"cell_type":"code","source":["class SphereFaceModel(torch.nn.Module):\n","    '''\n","    To train in a standard training loop make sure to modify the train function so you pass in the inputs and the labels\n","    i.e. output = model(images, labels)\n","    Experiment with different values of margin and scaler\n","    '''\n","    def __init__(self, model, margin=0.5, scaler=64, embedding_size=NotImplemented, num_classes=NotImplemented):\n","        super(SphereFaceModel, self).__init__()\n","        self.embedding_size = embedding_size\n","        self.num_classes = num_classes\n","\n","        # small number to avoid invalid arcCos values\n","        self.eps = 1e-7\n","\n","        # hyperparameters\n","        self.margin = margin\n","        self.scaler = scaler\n","\n","        # load classification model\n","        self.model = model\n","\n","        # Initializing the arcface linear layer with the weights of the classifier from the trained CNN\n","        self.SFL_linear = torch.nn.Linear(embedding_size, num_classes, bias=False) # Why set bias=False? Check out the paper.\n","        with torch.no_grad():\n","          self.SFL_linear.weight.copy_(self.model.cls_layer.weight)\n","\n","        # Initializing utility functions for normalization, arcCos, cos and onehot encoding\n","        self.normalizer = torch.nn.functional.normalize\n","        self.arcCos = torch.acos\n","        self.cos = torch.cos\n","        self.one_hot = torch.nn.functional.one_hot\n","\n","\n","    def forward(self, x, labels):\n","        # Get face embedding. Note that we pass return_feats=True to get the image's features and not the final logits.\n","        embedding = self.model(x, return_feats=True)\n","\n","        # TODO: Normalize face embedding using self.normalizer\n","        embedding = NotImplemented\n","\n","        # TODO: normalize linear layer weights.\n","        # NOTE: The normalized weights need to be wrapped in torch.nn.Parameter before assigning to AFL_linear.\n","        with torch.no_grad():\n","          self.SFL_linear.weight = torch.nn.Parameter(NotImplemented)\n","\n","        # TODO: take dot product to get cos theta, remember that Wx = ||W||||x||cos(\\theta) and ||W|| = 1, ||x|| = 1\n","        cosine = NotImplemented\n","\n","        # We clamp the values to be a little higher than -1 and a little lower than one so we don't get nan values when we call arccos\n","        cosine = torch.clamp(cosine, min=-1.0+self.eps, max=1.0-self.eps)\n","\n","        # TODO: get theta by performing arccos(cos(theta))\n","        theta = NotImplemented\n","\n","        # TODO: convert labels to one-hot\n","        one_hot_labels = NotImplemented\n","        # TODO: create a mask with m at positions with label 1 and 0 at positions with label 0\n","        margin_mask = NotImplemented\n","        # TODO: multiply margin m to theta\n","        theta_m = NotImplemented\n","\n","        # calculate the cosine value for theta with margin multiplied and scale with self.scaler\n","        logits = NotImplemented # this value is then passed to crossEntropyLoss in train loop to calculate sphereface loss\n","\n","        return logits\n"],"metadata":{"id":"4sHI80FZRGVP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Example of Training Procedure for Fine Tuning with ArcFace or SphereFace"],"metadata":{"id":"OT4Or38VIIc1"}},{"cell_type":"markdown","source":["There are small changes you will need to implement in your train function of the code (We recommend you make a new cell block or function for fine tune train). An example of the train function is given here.\n"],"metadata":{"id":"HyXqoH8xIU2U"}},{"cell_type":"code","source":["def finetune_train(model, dataloader, optimizer, criterion):\n","\n","    model.train()\n","\n","    # Progress Bar\n","    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n","\n","    num_correct = 0\n","    total_loss  = 0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        optimizer.zero_grad() # Zero gradients\n","\n","        images, labels = images.to(device), labels.to(device)\n","\n","        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n","            outputs = model(images, labels) #Why are we giving labels as well in here ?\n","            loss    = criterion(outputs, labels)\n","\n","        # Update no. of correct predictions & loss as we iterate\n","        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss      += float(loss.item())\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct = num_correct,\n","            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n","\n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update()\n","\n","        # TODO? Depending on your choice of scheduler,\n","        # You may want to call some schdulers inside the train function. What are these?\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss  = float(total_loss / len(dataloader))\n","\n","    return acc, total_loss"],"metadata":{"id":"l6JXTY_8I1-R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Center Loss"],"metadata":{"id":"qtyo95tbGRWa"}},{"cell_type":"markdown","source":["## Description:\n","Briefly speaking, Center Loss will decrease the variation of the feature cluster of each class.\n","\n","In other words, the objective of Center Loss is to minimize the intra-class variance of the output feature (the output of the model before being passed to the final classification layer).\n","\n","$$\\mathcal{L}_C = \\frac{1}{2}\\sum_{i=1}^{m}||\\pmb{x}_i-\\pmb{c}_{y_i}||_2^2$$\n","\n","Here $\\mathcal{L}_C$ denotes Center Loss, $\\pmb{x}_i$ denotes the feature vector of class $i$, $\\pmb{c}_{y_i}$ denotes the center of feature vectors within the class of $y_i$, and $m$ is the number of ($\\pmb{x}_i$,$y_i$) pairs.\n","\n","What we will actually implement here will be the mean of the loss, so that the scale of loss matches with cross entropy loss.\n","\n","$$\\mathcal{L}_C = \\frac{1}{2m}\\sum_{i=1}^{m}||\\pmb{x}_i-\\pmb{c}_{y_i}||_2^2$$\n","\n","However, it is too time-wasting to calculate the intra-class centers of ALL the data in every epoch. Therefore, Wen et.al decides to update the centers by batches. \"In each iteration, the centers are computed by\n","averaging the features of the corresponding classes (In this case, some of the\n","centers may not update).\"\n","\n","The centers are updated by a learning rate $\\alpha$ .\n","\n","$$\\frac{\\partial\\mathcal{L}_C}{\\partial\\pmb{x}_i} = \\pmb{x}_i-\\pmb{c}_{y_i}$$\n","\n","$$\\Delta\\pmb{c}_j = \\frac{\\sum_{i=1}^{m}\\delta(y_i=j)\\cdot(\\pmb{c}_i-\\pmb{x}_i)}{1+\\sum_{i=1}^{m}\\delta(y_i=j)}$$\n","\n","$$\\pmb{c}_{j}^{t+1}=\\pmb{c}_{j}^{t}-\\alpha\\cdot\\Delta\\pmb{c}_j$$\n","\n","Inside the class of Center Loss, you do not need to implement the update part. Update is handled by the optimizer, which means that you only need to calculate the loss."],"metadata":{"id":"Ur_4hAg1GVit"}},{"cell_type":"code","source":["class CenterLoss(nn.Module):\n","    \"\"\"Center Loss\n","        Center Loss Paper:\n","        https://ydwen.github.io/papers/WenECCV16.pdf\n","    Args:\n","        nn (_type_): _description_\n","    \"\"\"\n","    def __init__(self,\n","                 num_classes=NotImplemented, # TODO: What is the number of classes for our model?\n","                 feat_dim=NotImplemented, # TODO: What is the dimension of your output feature?\n","                 ) -> None:\n","        super(CenterLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.feat_dim = feat_dim\n","\n","        # I have written the initialization of centers for you here\n","        # Consider why the shape of centers is (num_classes, feat_dim)\n","        # You may want to adjust here if you want to test the program on cpu\n","        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n","\n","    def forward(self, x, labels):\n","        \"\"\"\n","        Args:\n","            x: feature matrix with shape (batch_size, feat_dim).\n","            labels: ground truth labels with shape (batch_size).\n","        \"\"\"\n","        centers = # TODO: Broadcast your self.centers so that centers[i] will contain the center of true label of x[i]\n","        dist = # TODO: Calculate the squared euclidian distances between your inputs and current centers\n","\n","        # Each element in dist is actually the Center Loss of each input\n","\n","        # Here you have to first wrap 'dist' inside torch.clamp() function, because log(0) will cause NaN output.\n","        # To avoid the 0 in 'dist', we will set the lower bound in 'dist' to a value that is close to 0\n","\n","        dist = torch.clamp(dist, min=1e-12, max=1e+12)\n","\n","        loss = # TODO: Calculate the mean loss across the batch.\n","\n","        return loss"],"metadata":{"id":"YIR4v2aGGVGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example in Training Procedure for Center Loss"],"metadata":{"id":"i4Xmv82xGblZ"}},{"cell_type":"markdown","source":["When you use FP16 in your training, there is a specific usage you have to follow if you use multiple losses in your training. Here is the example code for multiple loss training when you use Center Loss\n","\n","More detailed information in this link:\n","[link](https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-multiple-models-losses-and-optimizers)\n","\n","The hyperparameters you need to tune: loss weight $\\lambda$, loss learning rate $\\alpha$"],"metadata":{"id":"cnTfrRUbGegd"}},{"cell_type":"code","source":["center_loss = CenterLoss(num_classes=NotImplemented, feat_dim=NotImplemented)\n","optimizer_center_loss = torch.optim.SGD(center_loss.parameters(), lr = NotImplemented) # TODO: select a learning rate"],"metadata":{"id":"d8jsc8QsGSGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model: nn.Module,\n","          train_loader: Dataloader,\n","          optimizer: optim.Optimizer,\n","          optimizer_center_loss: optim.Optimizer,\n","          criterion: nn.Module,\n","          fine_tuning_loss: nn.Module, # here we are using Center Loss as our fine_tuning_loss\n","          loss_weight,\n","          scheduler: optim.lr_scheduler._LRScheduler,\n","          scaler: torch.cuda.amp.GradScaler,\n","          device):\n","\n","    num_correct = 0.0\n","    total_loss = 0.0\n","    model.train()\n","\n","    # Progress Bar\n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n","\n","    for i, (images, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        optimizer_center_loss.zero_grad()\n","\n","        images, labels = images.to(device), labels.to(device)\n","\n","        with torch.cuda.amp.autocast():\n","            outputs = model(images, return_feats=False)\n","            feats = model(images, return_feats=True)\n","            loss0 = criterion(outputs, labels) # calculate cross entropy loss from outputs and labels\n","            loss1 = loss_weight * fine_tuning_loss(feats, labels) # calculate weighted fine_tuning_loss (center loss) from feats and labels\n","            loss = loss0 + loss1\n","\n","        # Update no. of correct predictions & loss as we iterate\n","        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss += float(loss.item())\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct=num_correct,\n","            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n","\n","\n","        # backward loss0 to calculate gradients for model paramters\n","        # Hint: You have to pass retain_graph=True here, so that the scaler will remember this backward call\n","        scaler.scale(loss0).backward(retain_graph=True)\n","\n","        # backward loss1 to calculate gradients for fine_tuning_loss paramters\n","        scaler.scale(loss1).backward()\n","\n","        # update fine tuning loss' parameters\n","        # the paramerters should be adjusted according to the loss_weight you choose\n","        for parameter in fine_tuning_loss.parameters():\n","            parameter.grad.data *= (1.0 / loss_weight)\n","\n","        scaler.step(optimizer_center_loss)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        batch_bar.update() # Update tqdm bar\n","\n","        # if you use a scheduler to schedule your learning rate for Center Loss\n","        # scheduler_center_loss.step()\n","\n","        del images, labels, outputs, loss0, loss1\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","    acc = 100 * num_correct / (config['batch_size']* len(train_loader))\n","    total_loss = float(total_loss / len(train_loader))\n","\n","    return acc, total_loss"],"metadata":{"id":"qeBhsPltGj3R"},"execution_count":null,"outputs":[]}]}