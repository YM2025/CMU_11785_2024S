{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4P1:语言模型的【预测】和【生成】\n",
    "\n",
    "\n",
    "### 1. 作业概述\n",
    "这是课程的第一部分作业，主要是使用PyTorch进行训练。你将致力于训练语言模型，并对模型进行预测和生成任务的评估。\n",
    "\n",
    "### 2. 注意事项\n",
    "在完成作业的过程中，请仔细阅读代码，并关注待办事项（TODOs）。\n",
    "\n",
    "### 3. Jupyter Notebook的结构\n",
    "该笔记本的结构如下：\n",
    "\n",
    "- **Imports and installs（导入和安装）**：指定正确的数据路径，主要是运行相关代码。\n",
    "- **Datasets（数据集）**：完成TODO部分并运行。\n",
    "- **Dataloader（数据加载器）**：完成TODO部分并运行。\n",
    "- **Language model architecture（语言模型架构）**：根据写作要求，实施并定义你喜欢的模型架构。\n",
    "- **Dataloader, model, loss, optimizer, and scheduler definition（数据加载器、模型、损失函数、优化器和调度器的定义）**：定义数据加载器、模型、损失函数、优化器和调度器。\n",
    "- **Trainer class（训练类）**：与所有P2不同，我们为这次作业使用了Trainer类，需检查该类并完成训练函数。\n",
    "- **Wandb**：添加正确的API密钥。\n",
    "- **Experiments（实验）**：运行实验并记录最终的NLL（负对数似然）指标。\n",
    "- **Evaluation（评估）**：访问OpenAI API以获取最终的困惑度（Perplexity）指标。\n",
    "- **Submission（提交）**：为Autolab创建提交文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "#确保所有后续的 Matplotlib 图表都能在 Notebook 中显示\n",
    "%matplotlib inline \n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torchsummaryX\n",
    "import gc\n",
    "import wandb\n",
    "import yaml\n",
    "# import openai\n",
    "\n",
    "# Importing necessary modules from hw4\n",
    "# Update the path depending on how you choose to load the handout\n",
    "from tests_hw4 import get_prediction_nll, make_generation_text\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集\n",
    "\n",
    "我们将在 WikiText-2 语言建模数据集上训练 RNN 语言模型，该数据集已为 HW4p1 进行了预处理，并包含在模板压缩包中：\n",
    "- vocab.npy：包含词汇表中单词的 NumPy 文件\n",
    "- vocab.csv：列出词汇表的人可读 CSV 文件（ym备注： npy末尾还有SOS和EOS， csv中没有）\n",
    "- wiki.train.nyy：包含训练文本的NumPy文件\n",
    "\n",
    "我们还在 fixture 目录中提供了测试文件，这些文件将构成你将要实现的预测和生成函数的测试用例。你无需担心这些文件，处理它们的测试脚本已经为你设置好了。\n",
    "\n",
    "train 文件包含一个文章数组。每篇文章都是一个整数数组，与词汇表中的单词相对应。训练集中有 579 篇文章。\n",
    "例如，训练集中的第一篇文章包含 3803 个整数。第一篇文章的前 6 个整数是 [1420 13859 3714 7036 1420 1417]。在词汇表中查找这些整数，会发现第一行是： = Valkyria Chronicles III = \\<eol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  33280\n",
      "['!' '\"' '#' ... '～' '<sos>' '<eos>']\n"
     ]
    }
   ],
   "source": [
    "VOCAB       = np.load('dataset/vocab.npy')\n",
    "\n",
    "# We have also included <sos> and <eos> in the vocabulary for you\n",
    "# However in real life, you include it explicitly if not provided\n",
    "SOS_TOKEN   = np.where(VOCAB == '<sos>')[0][0] #获取第一个满足条件的索引值\n",
    "EOS_TOKEN   = np.where(VOCAB == '<eos>')[0][0]\n",
    "NUM_WORDS   = len(VOCAB) - 2 # Actual number of words in vocabulary\n",
    "\n",
    "print(\"Vocab length: \", len(VOCAB))\n",
    "print(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33278"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1420, 13859,  3714, ...,   813,    79,  1417])\n",
      " array([ 1420, 13463,  3117, ...,  8635,    79,  1417])]\n"
     ]
    }
   ],
   "source": [
    "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
    "dataset     = np.load('dataset/wiki.train.npy', allow_pickle=True)\n",
    "\n",
    "# 首先打印数据集的一部分，查看数据结构\n",
    "print(dataset[:2])  # 查看前2个样本\n",
    "\n",
    "# The dataset does not have <sos> and <eos> because they are just regular articles.\n",
    "# TODO: Add <sos> and <eos> to every article in the dataset.\n",
    "# Before doing so, try printing the dataset to see if they are words or integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总元素个数: 2075677\n"
     ]
    }
   ],
   "source": [
    "# 使用numpy的concatenate方法将所有数组连接起来，然后计算长度\n",
    "total_elements = np.concatenate(dataset).size\n",
    "\n",
    "print(\"总元素个数:\", total_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([33278,  1420, 13859, ...,    79,  1417, 33279], dtype=int64), array([33278,  1420, 13463, ...,    79,  1417, 33279], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# 为每篇文章首尾添加 <sos> 和 <eos> 标记\n",
    "dataset = [np.concatenate(([SOS_TOKEN], article, [EOS_TOKEN])) for article in dataset]\n",
    "\n",
    "# 检查是否成功加入\n",
    "print(dataset[:2])  # 查看前2个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation shapes    :  (128, 21) (128,)\n",
      "Test shapes          :  (128, 21)\n"
     ]
    }
   ],
   "source": [
    "# Loading the fixtures for validation and test - prediction\n",
    "fixtures_pred       = np.load('fixtures/prediction.npz')        # validation\n",
    "fixtures_pred_test  = np.load('fixtures/prediction_test.npz')   # test\n",
    "\n",
    "print(\"Validation shapes    : \", fixtures_pred['inp'].shape, fixtures_pred['out'].shape)\n",
    "print(\"Test shapes          : \", fixtures_pred_test['inp'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Gen Shapes    : (32, 21)\n",
      "Test Gen Shapes          : (128, 31)\n"
     ]
    }
   ],
   "source": [
    "# Loading the fixtures for validation and test - generation\n",
    "fixtures_gen        = np.load('fixtures/generation.npy')        # validation\n",
    "fixtures_gen_test   = np.load('fixtures/generation_test.npy')   # test\n",
    "\n",
    "print(\"Validation Gen Shapes    :\", fixtures_gen.shape)\n",
    "print(\"Test Gen Shapes          :\", fixtures_gen_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Input (inp): [33278 26096 26972 25821 14658 29325 32935 21820 25639 16134 31353 29092\n",
      "    79  6916    76 21415 14658 24911  1424 29456 29325]\n",
      "Input (inp): <sos> output port of a section will generally not be the same . However , for a mid @-@ series section\n",
      "Output (out): 72\n",
      "Output (out): (\n",
      "--------------------------------------------------\n",
      "Sample 2:\n",
      "Input (inp): [33278 14658 21076 21626 31353  6613  1419 10706 15340 25874 25949 31994\n",
      " 21626  2299  3952    79  1419    76  1184 31543  1242]\n",
      "Input (inp): <sos> a few from the Heavy <unk> Platoon and one or two from B Company . <unk> , 60 to 70\n",
      "Output (out): 24820\n",
      "Output (out): men\n",
      "--------------------------------------------------\n",
      "Sample 3:\n",
      "Input (inp): [33278  1419 15219 27351 25131 21415 32352 25871 31353 28863    76 31353\n",
      " 21201 31994 25821 32883 19278 21626 31353 25806  1424]\n",
      "Input (inp): <sos> <unk> also produced monitors for use on the rivers , the first two of which differed from the ocean @-@\n",
      "Output (out): 21959\n",
      "Output (out): going\n",
      "--------------------------------------------------\n",
      "Sample 4:\n",
      "Input (inp): [33278  6591 25821  1419    79 12269 22213 16176 18272 21626 14658 17765\n",
      " 32503 22968  9810 10184 21415 28860 15340 17575 21114]\n",
      "Input (inp): <sos> Head of <unk> . She had been converted from a commercial vessel in New Orleans for river and coastal fighting\n",
      "Output (out): 79\n",
      "Output (out): .\n",
      "--------------------------------------------------\n",
      "Sample 5:\n",
      "Input (inp): [33278 31353 21111 28343 18250    76 15659  1419 31326 27001 21415 20086\n",
      " 24118 25821  1419 27297 31543 22534 25537 21111    79]\n",
      "Input (inp): <sos> the fight remains controversial , as <unk> tested positive for elevated levels of <unk> prior to his next fight .\n",
      "Output (out): 1419\n",
      "Output (out): <unk>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Prediction Dev Input and Output\n",
    "# Optional TODO: You can try printing a few samples from the validation set which has both inputs and outputs\n",
    "\n",
    "# 打印验证集的前几个样本\n",
    "num_samples_to_print = 5  # 你想要打印的样本数量\n",
    "\n",
    "# 遍历并打印指定数量的样本\n",
    "for i in range(num_samples_to_print):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    \n",
    "    print(\"Input (inp):\", fixtures_pred['inp'][i])\n",
    "\n",
    "    \n",
    "    # 将输入序号转换为对应的英文单词或字符\n",
    "    input_words = [VOCAB[idx] for idx in fixtures_pred['inp'][i]]\n",
    "    print(\"Input (inp):\", ' '.join(input_words))  # 转换成字符串并打印\n",
    "    \n",
    "    # 打印输出标签\n",
    "    print(\"Output (out):\", fixtures_pred['out'][i])\n",
    "    print(\"Output (out):\", VOCAB[fixtures_pred['out'][i]])\n",
    "    \n",
    "    print(\"-\" * 50)  # 分隔线，方便阅读\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # 继承自 torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "        TODO: 在这里定义数据加载器的逻辑\n",
    "    \"\"\"\n",
    "    # TODO: 你可能还需要添加更多的参数。例如：序列长度（sequence length）\n",
    "    def __init__(self, dataset, batch_size, sequence_length = 10, shuffle=True, drop_last=False):\n",
    "\n",
    "        # 如果你还记得，这是定义数据加载器时需要提供的标准参数。\n",
    "        # 现在你只是自定义了你自己的数据加载器。\n",
    "        self.dataset = dataset  # 数据集，通常是由多个文章组成的列表\n",
    "        self.batch_size = batch_size  # 指每个批次（batch）中包含的序列（样本）的数量。每个序列的长度由 sequence_length 决定。\n",
    "        self.shuffle = shuffle  # 是否在每次迭代时打乱数据\n",
    "        self.drop_last = drop_last  # 是否丢弃最后一个不完整的批次\n",
    "        self.sequence_length = sequence_length  # 序列长度，决定每个样本的长度\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # 当你打印 len(loader) 时，你得到的是什么输出？你得到的是批次数量。\n",
    "        # 你的数据集有 (579, ) 篇文章，每篇文章包含指定数量的单词。\n",
    "        # 你将数据集连接起来，然后根据序列长度对其进行分批处理。\n",
    "\n",
    "        total_length = np.sum([self.dataset[i].shape[0] for i in range(len(self.dataset))])  # 计算数据集中所有单词的总长度\n",
    "        batch_count = total_length // (self.batch_size * self.sequence_length)  \n",
    "        return batch_count  # 计算数据集中可以生成多少个完整的批次。这个值用于控制训练的循环次数\n",
    "\n",
    "    def __iter__(self):\n",
    "        # TODO: 如果 shuffle 为 True，打乱数据\n",
    "        if self.shuffle:\n",
    "            # TODO\n",
    "            np.random.shuffle(self.dataset)  # 打乱数据集中的文章顺序\n",
    "\n",
    "        # TODO: 设置批次数量\n",
    "        num_batches = self.__len__()  # 获取批次数量\n",
    "        \n",
    "        batches = []  # 初始化批次列表\n",
    "        \n",
    "        self.dataset_concatenated = np.concatenate(self.dataset)  # 将数据集中的所有文章连接成一个长序列\n",
    "        for b in range(num_batches):\n",
    "            batch = self.dataset_concatenated[b*self.batch_size*self.sequence_length:(b+1)*self.batch_size*self.sequence_length + 1]  # 切片获取每个批次的数据\n",
    "            batches.append(batch)  # 将每个批次的数据添加到批次列表中\n",
    "            \n",
    "        if self.drop_last:\n",
    "            batches = batches[:-1]  # 如果 drop_last 为 True，丢弃最后一个不完整的批次\n",
    "            \n",
    "\n",
    "        # TODO: 将连接的数据集划分为输入和目标。它们如何变化？\n",
    "\n",
    "        # TODO: 将输入和目标重塑为批次（考虑最终的形状）\n",
    "\n",
    "        # TODO: 遍历批次并根据序列长度生成输入和目标批次\n",
    "        batch_idx = 0  # 初始化批次索引\n",
    "        \n",
    "        while batch_idx < batches.__len__(): #遍历 batches 列表中的每个批次\n",
    "            # [:-1] 表示去除当前批次中的最后一个元素, 因为在语言模型中，输入序列不需要包含最后一个元素（这是因为目标序列是要预测下一个词的）。\n",
    "            input = batches[batch_idx][:-1].reshape(self.batch_size, self.sequence_length)  # 重塑输入的形状\n",
    "            target = batches[batch_idx][1:].reshape(self.batch_size, self.sequence_length)  # 重塑目标的形状\n",
    "            batch_idx += 1  # 增加批次索引\n",
    "            yield torch.tensor(input), torch.tensor(target)  # 使用 yield 返回输入和目标张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对dataloder的备注\n",
    "\n",
    "1. **输入与目标的对齐**：\n",
    "   - 在语言模型中，输入序列用于预测目标序列。例如，输入是 `[The cat sat on the]`，目标是 `[cat sat on the mat]`。这种对齐方式使得模型能够学习到如何根据前面的单词预测下一个单词。\n",
    "   - 因此，输入序列去掉最后一个单词（`[:-1]`），而目标序列去掉第一个单词（`[1:]`），这样输入和目标是对齐的，输入的每个位置都对应目标的下一个位置。\n",
    "\n",
    "2. **确保批次大小和序列长度一致**：\n",
    "   - 使用 `reshape(self.batch_size, self.sequence_length)` 确保数据的形状是 `[batch_size, sequence_length]`，使得每个批次中的数据形状一致，这对于模型的批次处理非常重要。\n",
    "   - 这有助于训练过程中数据的批次化处理，使得模型能够在固定大小的批次上进行训练和更新权重。\n",
    "\n",
    "3. **生成器的使用**：\n",
    "   - `yield` 生成器的使用，使得数据可以逐批次地加载和处理，而不是一次性将所有数据加载到内存中。对于大数据集，这种方法非常有效，可以节省内存并提高处理效率。\n",
    "\n",
    "4. **`sequence_length` 的作用**：\n",
    "\n",
    "   - **定义输入序列的长度**：`sequence_length` 决定了每个输入序列中包含多少个单词或标记。例如，如果 `sequence_length` 设置为 `10`，那么每个输入序列将包含10个单词或标记。\n",
    "   - **序列切分**：对于一篇较长的文章，数据加载器会将其按 `sequence_length` 切分成多个序列。例如，如果文章有30个单词，`sequence_length` 为10，那么这篇文章将被切分成3个序列，每个序列包含10个单词。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10]) torch.Size([32, 10])\n",
      "x:  ['<sos>', '=', 'Leslie', 'Andrew', '=', '<eol>', 'Brigadier', 'Leslie', '<unk>', 'Andrew']\n",
      "y:  ['=', 'Leslie', 'Andrew', '=', '<eol>', 'Brigadier', 'Leslie', '<unk>', 'Andrew', 'VC']\n"
     ]
    }
   ],
   "source": [
    "# Some sanity checks\n",
    "\n",
    "dl = DataLoaderForLanguageModeling(\n",
    "    dataset     = dataset,\n",
    "    batch_size  = 32,\n",
    "    shuffle     = True,\n",
    "    drop_last   = True,\n",
    "    # Input Extra parameters here if needed\n",
    ")\n",
    "\n",
    "inputs, targets = next(iter(dl))\n",
    "print(inputs.shape, targets.shape)\n",
    "\n",
    "for x, y in dl:\n",
    "    print(\"x: \", [VOCAB[i] for i in x[0, :]]) # 打印第一个批次的第一个序列\n",
    "    print(\"y: \", [VOCAB[i] for i in y[0, :]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
      "[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]\n"
     ]
    }
   ],
   "source": [
    "a = [i for i in range(10,100)]\n",
    "\n",
    "for i in range(5):\n",
    "    print(a[i*10 : (i+1)*10 + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hid_dim):\n",
    "        \"\"\"\n",
    "        初始化语言模型的各个组件，包括嵌入层、LSTM单元（LSTMCells）和线性投影层。\n",
    "        \n",
    "        参数：\n",
    "        - vocab_size: 词汇表的大小，即模型能够处理的独特单词数量。\n",
    "        - embed_dim: 嵌入层的维度，即每个单词在嵌入空间中的表示维度。\n",
    "        - hid_dim: 隐藏层的维度，即LSTM单元中隐藏状态的大小。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义嵌入层，将词汇表中的每个单词映射到一个固定大小的向量空间\n",
    "        self.token_embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "        # 定义一系列LSTMCell单元，用于处理输入序列。这里使用了两个LSTMCell\n",
    "        # 每个LSTMCell负责处理一个时间步的输入\n",
    "        self.lstm_cells = torch.nn.Sequential(\n",
    "            torch.nn.LSTMCell(input_size=embed_dim, hidden_size=hid_dim),  # 第一个LSTMCell，输入为嵌入向量，输出为隐藏状态\n",
    "            torch.nn.LSTMCell(input_size=hid_dim, hidden_size=hid_dim)     # 第二个LSTMCell，输入为前一个LSTMCell的输出\n",
    "        )\n",
    "        self.hidden_dim = hid_dim  # 保存隐藏层的维度\n",
    "\n",
    "        # 定义线性层，将LSTM的输出映射到词汇表的概率分布，用于预测下一个单词\n",
    "        self.token_probability = torch.nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def rnn_step(self, embedding, hidden_states_list):\n",
    "        \"\"\"\n",
    "        对单个时间步的嵌入进行处理，更新LSTM单元的隐藏状态。\n",
    "        \n",
    "        参数：\n",
    "        - embedding: 当前时间步的嵌入向量\n",
    "        - hidden_states_list: LSTM单元的隐藏状态列表\n",
    "        \n",
    "        返回：\n",
    "        - embedding: 当前时间步的隐藏状态（作为下一个LSTM单元的输入）\n",
    "        - hidden_states_list: 更新后的隐藏状态列表\n",
    "        \"\"\"\n",
    "        # 初始化当前时间步的隐藏状态和细胞状态\n",
    "        hx = torch.zeros((embedding.shape[0], self.hidden_dim))  # 隐藏状态（batch_size, hidden_dim）\n",
    "        cx = torch.zeros((embedding.shape[0], self.hidden_dim))  # 细胞状态（batch_size, hidden_dim）\n",
    "        hidden_states_list[0] = (hx, cx)  # 存储初始的隐藏状态和细胞状态\n",
    "\n",
    "        # 遍历所有的LSTMCell，依次更新隐藏状态\n",
    "        for i in range(len(self.lstm_cells)):\n",
    "            hx, cx = self.lstm_cells[i](embedding, hidden_states_list[-1])\n",
    "            hidden_states_list[i] = (hx, cx)  # 更新当前LSTMCell的隐藏状态和细胞状态\n",
    "\n",
    "        # 返回当前LSTMCell的输出（即更新后的隐藏状态）和所有LSTMCell的隐藏状态列表\n",
    "        embedding = hx\n",
    "        return embedding, hidden_states_list\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        对输入序列进行推理，返回最后一个时间步的概率分布。\n",
    "        \n",
    "        参数：\n",
    "        - x: 输入的单词序列（可以是单个单词或一个序列）\n",
    "        \n",
    "        返回：\n",
    "        - prob: 最后一个时间步的概率分布\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x).long().to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():  # 使用推理模式，不计算梯度\n",
    "            prob, _ = self.forward(x)  # 调用前向传播，\n",
    "            return prob[:, -1, :] #获取最后一个时间步的概率分布\n",
    "\n",
    "    def generate(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        基于初始输入序列生成一个新序列，长度为指定的时间步数。\n",
    "        \n",
    "        参数：\n",
    "        - x: 初始输入的单词序列\n",
    "        - timesteps: 要生成的时间步数\n",
    "        \n",
    "        返回：\n",
    "        - generated_sequence: 生成的单词序列\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x).long().to(DEVICE)\n",
    "\n",
    "        # 首先通过前向传播获取初始序列的概率分布和隐藏状态列表\n",
    "        token_prob_dist, hidden_states_list = self.forward(x)\n",
    "        next_token = token_prob_dist[:,-1,:].argmax(dim = -1)  # 获取最可能的下一个单词（通过取概率分布的最大值）\n",
    "\n",
    "        next_token = next_token.reshape(token_prob_dist.shape[0], 1) # 维度[batch] → [batch, 1]\n",
    "        \n",
    "        generated_sequence = [next_token]  # 存储生成的单词序列\n",
    "        timesteps -= 1 # 因为之前已经预测了一次下一个单词。\n",
    "        with torch.inference_mode():\n",
    "            for t in range(timesteps):  # 遍历每一个时间步，逐步生成单词\n",
    "                # 使用上一个时间步生成的单词作为输入，更新隐藏状态\n",
    "                token_prob_dist, hidden_states_list = self.forward(next_token, hidden_states_list)\n",
    "                next_token = token_prob_dist.argmax(dim = -1)  # 获取当前时间步最可能的下一个单词\n",
    "                generated_sequence.append(next_token)  # 将生成的单词添加到序列中\n",
    "\n",
    "            # 将生成的单词序列堆叠为张量，形状为（batch_size, timesteps）\n",
    "            generated_sequence = torch.stack(generated_sequence, dim=1)\n",
    "        \n",
    "        # 去除维度为1的\n",
    "        generated_sequence = generated_sequence.squeeze(-1)\n",
    "        \n",
    "        return generated_sequence\n",
    "\n",
    "    def forward(self, x, hidden_states_list=None):\n",
    "        \"\"\"\n",
    "        模型的前向传播函数，处理输入序列，生成每个时间步的概率分布。\n",
    "        \n",
    "        参数：\n",
    "        - x: 输入的单词序列，形状为（batch_size, seq_len）\n",
    "        - hidden_states_list: LSTM单元的初始隐藏状态列表（如果有）\n",
    "        \n",
    "        返回：\n",
    "        - token_prob_distribution: 每个时间步的概率分布，形状为（batch_size, seq_len, vocab_size）\n",
    "        - hidden_states_list: 更新后的隐藏状态列表\n",
    "        \"\"\"\n",
    "        batch_size, timesteps = x.shape  # 获取批次大小和序列长度\n",
    "\n",
    "        # 存储所有时间步的概率分布\n",
    "        token_prob_distribution = []\n",
    "        # 初始化隐藏状态列表，如果没有提供初始隐藏状态\n",
    "        hidden_states_list = [None] * len(self.lstm_cells) if hidden_states_list is None else hidden_states_list\n",
    "\n",
    "        # 获取输入序列的嵌入表示，形状为（batch_size, seq_len, embed_dim）\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "\n",
    "        # 遍历每个时间步，逐步处理输入序列\n",
    "        for t in range(timesteps):\n",
    "            token_embedding_t = token_embeddings[:, t, :]  # 获取当前时间步的嵌入向量\n",
    "\n",
    "            # 通过LSTM单元处理当前时间步的嵌入，更新隐藏状态\n",
    "            rnn_out, hidden_states_list = self.rnn_step(token_embedding_t, hidden_states_list)\n",
    "\n",
    "            # 通过线性层将LSTM单元的输出映射到词汇表的概率分布\n",
    "            token_prob_dist_t = self.token_probability(rnn_out)\n",
    "\n",
    "            # 将当前时间步的概率分布添加到列表中\n",
    "            token_prob_distribution.append(token_prob_dist_t)\n",
    "\n",
    "        # 将所有时间步的概率分布堆叠为张量，形状为（batch_size, seq_len, vocab_size）\n",
    "        token_prob_distribution = torch.stack(token_prob_distribution, dim=1)\n",
    "\n",
    "        return token_prob_distribution, hidden_states_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loader, optimizer, criterion, scheduler, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "        初始化 Trainer 类，用于训练模型。\n",
    "\n",
    "        参数：\n",
    "        - model: 要训练的模型\n",
    "        - loader: 数据加载器，用于获取训练数据\n",
    "        - optimizer: 优化器，用于更新模型参数\n",
    "        - criterion: 损失函数，用于计算训练过程中的损失\n",
    "        - scheduler: 学习率调度器，用于调整学习率\n",
    "        - max_epochs: 最大训练轮数，默认为1\n",
    "        - run_id: 训练运行的标识符，用于保存和区分不同的实验结果\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        # 用于保存训练过程中的损失和生成的结果\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "\n",
    "    def calculate_loss(self, out, target):\n",
    "        \"\"\"\n",
    "        计算给定输出和目标之间的交叉熵损失。\n",
    "\n",
    "        参数：\n",
    "        - out: 模型的输出，形状为 (B, T, Vocab_size)，即批次大小、时间步和词汇表大小\n",
    "        - target: 目标序列，形状为 (B, T)，即批次大小和时间步\n",
    "\n",
    "        返回：\n",
    "        - loss: 计算得到的交叉熵损失\n",
    "        \"\"\"\n",
    "        # 将输出展平为二维张量，形状为 (B*T, Vocab_size)，保持词汇表大小不变\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        \n",
    "        # 将目标展平为一维张量，形状为 (B*T)\n",
    "        targets = target.reshape(-1)\n",
    "        \n",
    "        # 使用损失函数计算损失，损失函数会比较每个目标词汇与相应的概率分布\n",
    "        loss = self.criterion(out, targets)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        执行一个训练周期，遍历所有批次数据，并更新模型参数。\n",
    "        \"\"\"\n",
    "        self.model.train()  # 将模型设置为训练模式\n",
    "        self.model.to(DEVICE)  # 将模型移动到指定的设备（如 GPU）\n",
    "        epoch_loss = 0  # 初始化一个变量用于累积整个周期的损失\n",
    "        num_batches = 0  # 计数已处理的批次数量\n",
    "        \n",
    "        # 遍历数据加载器中的所有批次\n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n",
    "\n",
    "            # 清零优化器的梯度，准备计算新的梯度\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            out, _ = self.model(inputs)\n",
    "            \n",
    "            loss = self.calculate_loss(out, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # 优化器步骤，更新模型参数\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # # 调度器步骤，根据损失调整学习率\n",
    "            # self.scheduler.step(nll) #这里有问题，要放epoch层级里面，而不是batch层级里面\n",
    "\n",
    "            num_batches += 1 \n",
    "\n",
    "            # 将损失累加\n",
    "            loss = loss.item()\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        # 计算整个周期的平均损失\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1  # 增加已完成的周期计数\n",
    "        \n",
    "        # 打印训练信息\n",
    "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
    "              % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        # 将本周期的损失添加到损失记录中\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        执行模型的测试，评估模型在验证集上的表现。\n",
    "        \"\"\"\n",
    "        self.model.eval()  # 将模型设置为评估模式\n",
    "        predictions = self.model.predict(fixtures_pred['inp']).detach().cpu().numpy()  # 获取预测结果\n",
    "        # self.predictions.append(predictions)\n",
    "\n",
    "        # 计算验证损失（负对数似然）\n",
    "        nll = get_prediction_nll(predictions, fixtures_pred['out'])\n",
    "        ##self.val_losses.append(nll)\n",
    "        \n",
    "        \n",
    "        generated_logits = self.model.generate(fixtures_gen, 10).detach().cpu().numpy()  # 生成10个单词的预测\n",
    "        ##generated_logits_test = self.model.generate(fixtures_gen_test, 10).detach().cpu().numpy()\n",
    "        \n",
    "        # 生成的结果\n",
    "        generated = make_generation_text(fixtures_gen, generated_logits, VOCAB)\n",
    "        ##generated_test = make_generation_text(fixtures_gen_test, generated_logits_test, VOCAB)\n",
    "\n",
    "        # self.generated.append(generated)\n",
    "        # self.generated_test.append(generated_test)\n",
    "        # self.generated_logits.append(generated_logits)\n",
    "        # self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # # 生成测试数据的预测结果\n",
    "        # predictions_test = self.model.predict(fixtures_pred_test['inp']).detach().cpu().numpy()\n",
    "        # self.predictions_test.append(predictions_test)\n",
    "        \n",
    "        # 打印验证信息\n",
    "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n",
    "              % (self.epochs, self.max_epochs, nll))\n",
    "        \n",
    "        return nll, generated\n",
    "\n",
    "    \n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        保存模型的状态字典、预测结果以及生成的结果到指定路径。\n",
    "        这个方法不需要修改。\n",
    "        \"\"\"\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()}, model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        \n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "configs = dict(\n",
    "    batch_size  = 32,\n",
    "    num_epochs  = 10, # 10 or 20 epochs should be enough given the model is good\n",
    "    sequence_length = 10, #定义切分文章时，每几个词构成一句话用来训练\n",
    "    init_lr     = 0.001 # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (token_embedding): Embedding(33280, 128)\n",
      "  (lstm_cells): Sequential(\n",
      "    (0): LSTMCell(128, 128)\n",
      "    (1): LSTMCell(128, 128)\n",
      "  )\n",
      "  (token_probability): Linear(in_features=128, out_features=33280, bias=True)\n",
      ")\n",
      "torch.Size([64, 10]) torch.Size([64, 10])\n",
      "3245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LanguageModel                            [64, 10, 33280]           --\n",
       "├─Embedding: 1-1                         [64, 10, 128]             4,259,840\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-1                     [64, 128]                 132,096\n",
       "│    └─LSTMCell: 2-2                     [64, 128]                 132,096\n",
       "├─Linear: 1-3                            [64, 33280]               4,293,120\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-3                     [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-4                     [64, 128]                 (recursive)\n",
       "├─Linear: 1-5                            [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-5                     [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-6                     [64, 128]                 (recursive)\n",
       "├─Linear: 1-7                            [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-7                     [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-8                     [64, 128]                 (recursive)\n",
       "├─Linear: 1-9                            [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-9                     [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-10                    [64, 128]                 (recursive)\n",
       "├─Linear: 1-11                           [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-11                    [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-12                    [64, 128]                 (recursive)\n",
       "├─Linear: 1-13                           [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-13                    [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-14                    [64, 128]                 (recursive)\n",
       "├─Linear: 1-15                           [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-15                    [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-16                    [64, 128]                 (recursive)\n",
       "├─Linear: 1-17                           [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-17                    [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-18                    [64, 128]                 (recursive)\n",
       "├─Linear: 1-19                           [64, 33280]               (recursive)\n",
       "├─Sequential: 1-20                       --                        (recursive)\n",
       "│    └─LSTMCell: 2-19                    [64, 128]                 (recursive)\n",
       "│    └─LSTMCell: 2-20                    [64, 128]                 (recursive)\n",
       "├─Linear: 1-21                           [64, 33280]               (recursive)\n",
       "==========================================================================================\n",
       "Total params: 8,817,152\n",
       "Trainable params: 8,817,152\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 24.66\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 172.36\n",
       "Params size (MB): 35.27\n",
       "Estimated Total Size (MB): 207.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model       = LanguageModel(vocab_size = len(VOCAB), embed_dim = 128, hid_dim = 128)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loader      = DataLoaderForLanguageModeling(dataset, configs['batch_size'], sequence_length=configs['sequence_length']) \n",
    "\n",
    "criterion   = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "optimizer   = torch.optim.Adam(model.parameters(), lr=configs['init_lr'])\n",
    "# TODO: Define the optimizer. Adam/AdamW usually works good for this HW\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = configs['num_epochs'], eta_min=1e-6)\n",
    "\n",
    "print(model)\n",
    "inputs, targets = next(iter(loader))\n",
    "print(inputs.shape, targets.shape)\n",
    "print(loader.__len__())\n",
    "import torchinfo\n",
    "torchinfo.summary(model.to(DEVICE), input_data=inputs.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1724732305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "run_id = str(int(time.time()))\n",
    "\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)\n",
    "\n",
    "# The object of the Trainer class takes in everything\n",
    "trainer = Trainer(\n",
    "    model       = model, \n",
    "    loader      = loader, \n",
    "    optimizer   = optimizer,\n",
    "    criterion   = criterion, \n",
    "    scheduler   = scheduler,\n",
    "    max_epochs  = configs['num_epochs'], \n",
    "    run_id      = run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aeafe47dcfe41e3aae381a3c7932957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [1/10] \tLoss: 6.5523 \tLr: 0.001000\n",
      "[VAL] \tEpoch [1/10] \tLoss: 5.3584\n",
      "5.3584466\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | not to the <unk> of the <unk> , and the\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> The first time of the first\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ year @-@ year @-@ year @-@ year\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> , and the <unk>\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ year @-@ <unk> @-@ <unk> <unk> , <unk> ,\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , the first time , the <unk> of the <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | , the <unk> of the <unk> of the <unk> ,\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | of the <unk> , and the <unk> of the <unk>\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> , and the <unk> of the <unk> ,\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | , the <unk> of the <unk> of the <unk> ,\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | the first time , the first time , the <unk>\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | to the <unk> of the <unk> , and the <unk>\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> , and the <unk> of the <unk>\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the first time , the <unk> of the <unk> of\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the <unk> of the <unk> of the <unk> ,\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | , and the <unk> of the <unk> , and the\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | , and the <unk> of the <unk> , and the\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first time of the first time , the first\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @.@ 5 million ) , and the <unk> of the\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the <unk> of the <unk> of the <unk> , and\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> of the <unk> , and the <unk>\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | <unk> ) , and the <unk> of the <unk> of\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | first time , the <unk> of the <unk> of the\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the first time , the <unk> of the <unk> of\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> , and the\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | , and the <unk> of the <unk> , and the\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | , the <unk> of the <unk> of the <unk> ,\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first time of the first time\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = = = = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . The first time of the <unk> of the <unk>\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the <unk> of the <unk> of the <unk> of\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , the first time , the <unk> of the <unk>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2237eecc02584bd5a250b134cfbae12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [2/10] \tLoss: 5.8059 \tLr: 0.000976\n",
      "[VAL] \tEpoch [2/10] \tLoss: 5.1629\n",
      "5.1629086\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used to be a <unk> <unk> . <eol> =\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = <eol> The first time of the <unk> of\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old <unk> . <eol> = = =\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and <unk> , and <unk> , and <unk> , and\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ year @-@ old <unk> . <eol> = = =\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> <unk> , <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | . <eol> = = = = <unk> = = =\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | of the <unk> . <eol> = = = = <unk>\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> of the <unk> . <eol> = = =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = = <unk> = = =\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time , the <unk> of the <unk>\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" . <eol> = = = =\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . <eol> = = = = <unk>\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , and the <unk> of the <unk> <unk> , and\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | , and the <unk> of the <unk> <unk> , <unk>\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | , and the <unk> of the <unk> <unk> , <unk>\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first time of the <unk> of the <unk> <unk>\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @.@ 5 million ) . <eol> = = = =\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the <unk> of the <unk> of the <unk> <unk> ,\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 1 @.@ 5 km ) . <eol> = = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | United States , and the <unk> of the <unk> <unk>\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , and the <unk> of the <unk>\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> <unk> , <unk>\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | , and the <unk> of the <unk> <unk> , and\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | , the <unk> of the <unk> <unk> , <unk> ,\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first time of the <unk> of\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = = <unk> = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = = <unk> = = =\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the <unk> of the <unk> of the <unk> <unk>\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , and the <unk> of the <unk> <unk> , <unk>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b192e52652f4359967ca98c781397eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [3/10] \tLoss: 5.5170 \tLr: 0.000905\n",
      "[VAL] \tEpoch [3/10] \tLoss: 5.0101\n",
      "5.010111\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used as a <unk> , and the <unk> of\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = <eol> The first two @-@ thirds of the\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old <unk> , and the <unk> of\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> , and the <unk>\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ based on the game . <eol> = = =\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> , and the\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | , the <unk> of the <unk> , and the <unk>\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . The game was a <unk> of the <unk> <unk>\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> . The ship was a <unk> of the\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . The ship was a <unk> of the <unk> <unk>\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time , the first time in the\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | to be \" a \" <unk> \" , and \"\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . The ship was a <unk> of\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . The ship was a <unk> of\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the <unk> of the <unk> , and the <unk>\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | 's <unk> , and the <unk> of the <unk> ,\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | , and the <unk> of the <unk> , and the\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first two @-@ thirds of the game was the\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 yd ( 1 @.@ 5 m ) .\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the game was \" <unk> \" , and \" <unk>\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the end of the game . The ship was\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 3 @.@ 5 m ) . The ship was a\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | game , the first time in the United States ,\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , and the first time in the\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> . The game\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II . The ship was a <unk> , and the\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also used as a <unk> , and the <unk>\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first two @-@ thirds of the\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" , and the \" <unk> \" , and \"\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . The ship was a <unk> of the <unk> <unk>\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the game was \" a \" <unk> \" ,\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , the <unk> of the <unk> , and the <unk>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e134b053d17448faaf6162b214f8242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [4/10] \tLoss: 5.3267 \tLr: 0.000794\n",
      "[VAL] \tEpoch [4/10] \tLoss: 5.0033\n",
      "5.0032597\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used as a <unk> , and the <unk> of\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = <eol> The first of the <unk> of the\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old version of the <unk> of the\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> <unk> , <unk> ,\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ like @-@ <unk> . <eol> = = = <unk>\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> <unk> , <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the United States . The <unk> of the <unk>\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> of the <unk> . <eol> = = =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time the first time in the United\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , and the <unk> of the\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . The <unk> of the <unk> <unk>\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the <unk> of the <unk> <unk> , <unk> ,\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | 's <unk> , and the <unk> of the <unk> <unk>\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> , the <unk> of the <unk> <unk>\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first of the <unk> of the <unk> is the\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @.@ <unk> ° W / h ) . <eol> =\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the <unk> of the <unk> <unk> , the <unk> of\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> . <eol> = = = <unk> =\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 3 @.@ <unk> ° W ) . <eol> = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | United States , the <unk> of the <unk> <unk> ,\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States . The <unk> of the <unk> <unk>\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , the <unk> of the <unk> <unk> , <unk> ,\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II ( <unk> ) , and the <unk> of the\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also used as a result of the <unk> .\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first of the <unk> of the\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = <unk> = = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the <unk> of the <unk> <unk> , the <unk>\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , the <unk> of the <unk> <unk> , <unk> ,\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5406d1c0e049449290bb82f61607e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [5/10] \tLoss: 5.1899 \tLr: 0.000655\n",
      "[VAL] \tEpoch [5/10] \tLoss: 4.9721\n",
      "4.9720583\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used to be the <unk> of the <unk> .\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = <eol> The first of the season , the\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old @-@ time with a <unk> <unk>\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> <unk> , and the\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ day @-@ up . <eol> = = = <unk>\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> <unk> , <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | to the <unk> . <eol> = = = <unk> =\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> . <eol> = = = <unk> = =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time in the United States , the\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , and the \" <unk> \"\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . <eol> = = = <unk> =\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . <eol> = = = <unk> =\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the first time in the United States , and\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | <unk> , and the <unk> of the <unk> <unk> ,\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> , and the <unk> of the <unk>\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first of the season , the first time in\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 @,@ 000 . <eol> = = = <unk>\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the first time in the United States , the first\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> . <eol> = = = <unk> =\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 1 @,@ 900 ft ) . <eol> = = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | United States , the first time in the United States\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , and the first time in the\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> . <eol> =\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II of the season . <eol> = = = <unk>\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also used in the <unk> <unk> . <eol> =\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first of the season , the\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" , and the <unk> of the <unk> <unk> ,\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the \" <unk> \" , \" The <unk> \"\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , the first time in the United States , the\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a511e36040c480088da4969a837b8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [6/10] \tLoss: 5.0851 \tLr: 0.000501\n",
      "[VAL] \tEpoch [6/10] \tLoss: 4.8943\n",
      "4.8942766\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used in the <unk> <unk> . <eol> = =\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> The first of the first time\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old @-@ <unk> , and the <unk>\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> <unk> , and the\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ year @-@ old . <eol> = = = =\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> <unk> , <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the United States . <eol> = = = =\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = = <unk> = = =\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> . <eol> = = = = <unk> =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = = <unk> = = =\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time in the United States , the\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , and \" <unk> \" .\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . <eol> = = = = <unk>\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . <eol> = = = = <unk>\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the first time in the United States , and\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | Jie 's <unk> , and the <unk> of the <unk>\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> , the <unk> of the <unk> <unk>\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first of the season , the first time in\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 <unk> ) , and the <unk> of the\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the <unk> of the <unk> , the <unk> of the\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> . <eol> = = = = <unk>\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 1 @.@ 6 km ) . <eol> = = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | United States , the <unk> of the <unk> , the\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , the <unk> of the <unk> ,\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> . <eol> =\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II of the Year . <eol> = = = =\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also used in the United States . <eol> =\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first of the first time in\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = = <unk> = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = = <unk> = = =\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the <unk> of the <unk> , the <unk> of\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , and the <unk> of the <unk> , the <unk>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3709a11e362c421c8fdc9eb8b77f0960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [7/10] \tLoss: 5.0053 \tLr: 0.000346\n",
      "[VAL] \tEpoch [7/10] \tLoss: 4.8893\n",
      "4.889266\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used to be the first to be a <unk>\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = <eol> The first of the season , the\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old @-@ time \" . <eol> =\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> <unk> , and the\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ based on the <unk> of the <unk> . <eol>\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> , the <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the United States . <eol> = = = <unk>\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> of the <unk> . <eol> = = =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time in the United States , the\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , which is a \" <unk>\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . <eol> = = = <unk> =\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the first time in the United States , and\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | Jie 's <unk> , and the <unk> of the <unk>\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> , and the <unk> of the <unk>\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first of the season , the first time in\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 . <eol> = = = <unk> = =\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the <unk> of the <unk> , the <unk> of the\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 1 @.@ 8 km ) . <eol> = = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | <unk> of the <unk> , the <unk> of the <unk>\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , the <unk> of the <unk> ,\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> . <eol> =\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II ( <unk> ) , and the <unk> of the\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also used for the first time in the United\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first of the first time in\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = <unk> = = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the \" <unk> \" , \" The <unk> \"\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , and the first time in the United States ,\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7edc1bc8163431fafa3bf7da9baf247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [8/10] \tLoss: 4.9447 \tLr: 0.000207\n",
      "[VAL] \tEpoch [8/10] \tLoss: 4.8991\n",
      "4.8990984\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used to be a <unk> , and the <unk>\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> The first of the season ,\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old @-@ game against the <unk> .\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ year contract with the <unk> of the <unk> .\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> , the <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the United States . <eol> = = = =\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = = <unk> = = =\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> of the <unk> . <eol> = = =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = = <unk> = = =\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time in the first season , the\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , which was \" <unk> \"\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . <eol> = = = = <unk>\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the final . <eol> = = = = <unk> =\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the first time in the United States . <eol>\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | Jie 's <unk> . <eol> = = = <unk> =\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> , the <unk> of the <unk> ,\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first of the season , the first time in\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 . <eol> = = = = = =\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the first time in the first season of the season\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> . <eol> = = = = <unk>\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 1 @.@ 6 km ) . <eol> = = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | <unk> , the <unk> of the <unk> , and the\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the early 1990s , and the first time in the\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> . <eol> =\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II ( <unk> ) , and the <unk> of the\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also used for the first time in the United\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first of the season , the\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = = <unk> = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | in the United States . <eol> = = = =\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the \" <unk> \" , and \" <unk> \"\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , and the first time in the United States ,\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af1069931a042868ffc8a09e30aea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [9/10] \tLoss: 4.9041 \tLr: 0.000096\n",
      "[VAL] \tEpoch [9/10] \tLoss: 4.8921\n",
      "4.892123\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | not only to be the first to be a <unk>\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = <eol> The first of the season , the\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old @-@ time with a <unk> <unk>\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> <unk> , and <unk>\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ day Category 1 hurricane . The first time in\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> , the <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the United States . The first time in the\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> of the <unk> . <eol> = = =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time in the United States , the\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , and \" <unk> \" .\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the first round of the season . <eol> = =\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the first time in the United States . <eol>\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | Jie 's <unk> . <eol> = = = <unk> =\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> , the <unk> of the <unk> of\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first of the season , the first time in\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 <unk> ) , and the <unk> of the\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the first time in the first half of the season\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 1 @.@ 6 km ) . <eol> = = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | <unk> of the <unk> , the <unk> of the <unk>\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , and the first time in the\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> . <eol> =\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II of the Year . <eol> = = = <unk>\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were not in the United States . <eol> = =\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first of the season , the\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = <unk> = = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the \" <unk> \" , \" The <unk> \"\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , and the first time in the United States ,\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbd93fd2a804e058772136da51f7c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [10/10] \tLoss: 4.8811 \tLr: 0.000025\n",
      "[VAL] \tEpoch [10/10] \tLoss: 4.8904\n",
      "4.8903923\n",
      "Input | Output #0: <sos> while the group was en route , but only three were ultimately able to attack . None of them were | also used to be the first to be a <unk>\n",
      "Input | Output #1: <sos> <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = <eol> The first of the season , the\n",
      "Input | Output #2: <sos> 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ year @-@ old @-@ time with a <unk> <unk>\n",
      "Input | Output #3: <sos> , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and the <unk> of the <unk> <unk> , and the\n",
      "Input | Output #4: <sos> Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ day Category 1 hurricane . The first time in\n",
      "Input | Output #5: <sos> the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and the <unk> of the <unk> , the <unk>\n",
      "Input | Output #6: <sos> <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the United States . The first time in the\n",
      "Input | Output #7: <sos> The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #8: <sos> by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> of the <unk> . <eol> = = =\n",
      "Input | Output #9: <sos> Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #10: <sos> = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the first time in the United States , the\n",
      "Input | Output #11: <sos> of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , which was \" <unk> \"\n",
      "Input | Output #12: <sos> — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . <eol> = = = <unk> =\n",
      "Input | Output #13: <sos> And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . <eol> = = = <unk> =\n",
      "Input | Output #14: <sos> Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , the first time in the United States , and\n",
      "Input | Output #15: <sos> Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | Jie 's <unk> . <eol> = = = <unk> =\n",
      "Input | Output #16: <sos> Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> , the <unk> of the <unk> of\n",
      "Input | Output #17: <sos> by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first of the season , the first time in\n",
      "Input | Output #18: <sos> alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 <unk> ) , and the <unk> of the\n",
      "Input | Output #19: <sos> while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the first time in the United States , the storm\n",
      "Input | Output #20: <sos> first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> of the <unk> . <eol> = =\n",
      "Input | Output #21: <sos> Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 1 @.@ 6 km ) . <eol> = = =\n",
      "Input | Output #22: <sos> \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | <unk> of the <unk> , the <unk> of the <unk>\n",
      "Input | Output #23: <sos> were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , and the <unk> of the <unk>\n",
      "Input | Output #24: <sos> port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , and the <unk> of the <unk> . <eol> =\n",
      "Input | Output #25: <sos> T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | II of the Year . <eol> = = = <unk>\n",
      "Input | Output #26: <sos> South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were the first to be the first to be a\n",
      "Input | Output #27: <sos> Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first of the season , the\n",
      "Input | Output #28: <sos> deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = <unk> = = =\n",
      "Input | Output #29: <sos> , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #30: <sos> production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the \" <unk> \" , \" The <unk> \"\n",
      "Input | Output #31: <sos> and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , and the first time in the United States ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the experiments loop. \n",
    "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
    "#   * You might be overlapping batches \n",
    "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
    "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
    "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
    "#   * Your length calculation in the dataloader might be wrong\n",
    "# If you haven't had biryani, try it :D \n",
    "\n",
    "# %%time\n",
    "\n",
    "\n",
    "for epoch in range(configs['num_epochs']):\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    nll, generated = trainer.test()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(nll)\n",
    "    print(generated) #打印输入句子及其该句未来10个词的预测。\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
